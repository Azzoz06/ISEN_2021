{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten digit recognition with MNIST using IBM Watson Studio Jupyter Notebook in Python & Watson Machine Learning capabilities & Toolbox (ART) - for Data Scientist.\n",
    "This tutorial shows IBM Watson Studio framework capabilities to create Jupyter notebook leveraging Keras ML framework to create a convolutional neural network (CNN) model that is build & trained & improved in term of robustness & accuracy with IBM Watson Machine Learning Toolbox ART & IBM Watson ML capabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This tutorial will leverage **IBM ART** *(Adversarial Robustness Toolbox)* open source library\n",
    "available at https://github.com/IBM/adversarial-robustness-toolbox\n",
    "\n",
    "This is a library dedicated to adversarial machine learning.\n",
    "Its purpose is to allow rapid crafting and analysis of attacks and defense methods for machine learning models.\n",
    "The Adversarial Robustness Toolbox provides an implementation for many state-of-the-art methods for attacking and defending classifiers.\n",
    "\n",
    "The ART toolbox is developed with the goal of helping developers better understand:\n",
    " * Measuring model robustness\n",
    " * Model hardening\n",
    " * Runtime detection\n",
    "\n",
    "For more information you can read `Adversarial Robustness Toolbox v0.3.0` IBM research publication from Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh~Ngoc and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian and Edwards, Ben\n",
    "available here: https://arxiv.org/pdf/1807.01069"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup \n",
    "Install base Data Science libraries, then Keras and ART into our environment and import all required classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import base Data Science libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num=None, figsize=(8, 6), dpi=100, facecolor='w', edgecolor='k')\n",
    "%matplotlib inline\n",
    "\n",
    "# Import Keras\n",
    "import keras\n",
    "print(\"Keras version :\",keras.__version__)\n",
    "from keras.models import load_model\n",
    "import keras.backend as k\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import warnings\n",
    "warnings.resetwarnings() # Maybe somebody else is messing with the warnings system?\n",
    "warnings.filterwarnings('ignore') # Ignore everything\n",
    "\n",
    "# Install ART and import packages \n",
    "!pip install adversarial-robustness-toolbox\n",
    "from art.utils import load_dataset\n",
    "from art.classifiers import KerasClassifier\n",
    "from art.attacks.fast_gradient import FastGradientMethod\n",
    "from art.attacks.newtonfool import NewtonFool\n",
    "from art.attacks.iterative_method import BasicIterativeMethod\n",
    "from art.defences.adversarial_trainer import AdversarialTrainer\n",
    "\n",
    "## Optional: set random seeds for reproducibility\n",
    "#np.random.seed(99)\n",
    "#import tensorflow as tf\n",
    "#tf.set_random_seed(42);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define utility functions \n",
    "Utility functions to manipulate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few utility functions\n",
    "\n",
    "def setupSubPlots(numCols,maxRows):\n",
    "    numRows=1+(maxRows-1)//numCols\n",
    "    return plt.subplots(numRows,numCols,squeeze=False,figsize=(20,numRows/3*4))\n",
    "\n",
    "def plotImage(img,axs,ix,numCols,title=None):\n",
    "    ax=axs[ix//numCols][ix%numCols]\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    if title: \n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks([])\n",
    "    return ax.imshow(img.squeeze())\n",
    "\n",
    "def showImages(imgTable, limit, numCols=15, legend=None):\n",
    "    ''' Image plotting function, using a set number of columns\n",
    "    '''\n",
    "    limit=min(limit,len(imgTable))\n",
    "    if limit<numCols: numCols=limit\n",
    "    fig, axs = setupSubPlots(numCols,limit)\n",
    "    for ix,img in enumerate(imgTable):\n",
    "        if ix>=limit: break\n",
    "        plotImage(img,axs,ix,numCols)\n",
    "    if legend:\n",
    "        fig.suptitle(legend, fontsize=16)\n",
    "    return\n",
    "\n",
    "def showImagesAccuracy(description,images, labels, predictions, maxShown=30, numCols=15):\n",
    "    ''' Check Images prediction accuracy, count and display mismatches\n",
    "    '''\n",
    "    idx = 0\n",
    "    errorsCount = 0\n",
    "    shown = 0\n",
    "    fig, axs = setupSubPlots(numCols,maxShown)\n",
    "    for idx,img in enumerate(images):\n",
    "        predicted = np.argmax(predictions[idx])\n",
    "        actual = np.argmax(labels[idx])\n",
    "        if predicted != actual:\n",
    "            errorsCount += 1\n",
    "            if shown < maxShown:\n",
    "                # Plotting first samples of MNIST\n",
    "                plotImage(img,axs,shown,numCols,\"{}->{}\".format(actual,predicted))\n",
    "                shown += 1\n",
    "    # Compute accuracy as a percentage         \n",
    "    accuracy = (100-(errorsCount/len(images))*100)\n",
    "    legend=\"{}\\nSucccess rate: {:.2f}%\".format(description,accuracy)\n",
    "    print(legend)\n",
    "    #fig.suptitle(legend)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST data set utility loading code\n",
    "This code is used to load the MNISt dataset and prepare it for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file1(filename, url, path=None, extract=False):\n",
    "    \"\"\"\n",
    "    Downloads a file from a URL if it not already in the cache. The file at indicated by `url` is downloaded to the\n",
    "    path `path` (default is ~/.art/data). and given the name `filename`. Files in tar, tar.gz, tar.bz, and zip formats\n",
    "    can also be extracted. This is a simplified version of the function with the same name in Keras.\n",
    "\n",
    "    :param filename: Name of the file.\n",
    "    :type filename: `str`\n",
    "    :param url: Download URL.\n",
    "    :type url: `str`\n",
    "    :param path: Folder to store the download. If not specified, `~/.art/data` is used instead.\n",
    "    :type: `str`\n",
    "    :param extract: If true, tries to extract the archive.\n",
    "    :type extract: `bool`\n",
    "    :return: Path to the downloaded file.\n",
    "    :rtype: `str`\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if path is None:\n",
    "        from art import DATA_PATH\n",
    "        path_ = os.path.expanduser(DATA_PATH)\n",
    "    else:\n",
    "        path_ = os.path.expanduser(path)\n",
    "    if not os.access(path_, os.W_OK):\n",
    "        path_ = os.path.join('./', '.art')\n",
    "    if not os.path.exists(path_):\n",
    "        os.makedirs(path_)\n",
    "\n",
    "    if extract:\n",
    "        extract_path = os.path.join(path_, filename)\n",
    "        full_path = extract_path + '.tar.gz'\n",
    "    else:\n",
    "        full_path = os.path.join(path_, filename)\n",
    "\n",
    "    # Determine if dataset needs downloading\n",
    "    download = not os.path.exists(full_path)\n",
    "\n",
    "    if download:\n",
    "        # logger.info('Downloading data from %s', url)\n",
    "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
    "        try:\n",
    "            try:\n",
    "                from six.moves.urllib.error import HTTPError, URLError\n",
    "                from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "                urlretrieve(url, full_path)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(url, e.code, e.msg))\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(url, e.errno, e.reason))\n",
    "        except (Exception, KeyboardInterrupt):\n",
    "            if os.path.exists(full_path):\n",
    "                os.remove(full_path)\n",
    "            raise\n",
    "\n",
    "    if extract:\n",
    "        if not os.path.exists(extract_path):\n",
    "            _extract(full_path, path_)\n",
    "        return extract_path\n",
    "\n",
    "    return full_path\n",
    "\n",
    "\n",
    "def preprocess(x, y, nb_classes=10, max_value=255):\n",
    "    \"\"\"Scales `x` to [0, 1] and converts `y` to class categorical confidences.\n",
    "\n",
    "    :param x: Data instances\n",
    "    :type x: `np.ndarray`\n",
    "    :param y: Labels\n",
    "    :type y: `np.ndarray`\n",
    "    :param nb_classes: Number of classes in dataset\n",
    "    :type nb_classes: `int`\n",
    "    :param max_value: Original maximum allowed value for features\n",
    "    :type max_value: `int`\n",
    "    :return: rescaled values of `x`, `y`\n",
    "    :rtype: `tuple`\n",
    "    \"\"\"\n",
    "    x = x.astype('float32') / max_value\n",
    "    y = to_categorical(y, nb_classes)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def to_categorical(labels, nb_classes=None):\n",
    "    \"\"\"\n",
    "    Convert an array of labels to binary class matrix.\n",
    "\n",
    "    :param labels: An array of integer labels of shape `(nb_samples,)`\n",
    "    :type labels: `np.ndarray`\n",
    "    :param nb_classes: The number of classes (possible labels)\n",
    "    :type nb_classes: `int`\n",
    "    :return: A binary matrix representation of `y` in the shape `(nb_samples, nb_classes)`\n",
    "    :rtype: `np.ndarray`\n",
    "    \"\"\"\n",
    "    labels = np.array(labels, dtype=np.int32)\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(labels) + 1\n",
    "    categorical = np.zeros((labels.shape[0], nb_classes), dtype=np.float32)\n",
    "    categorical[np.arange(labels.shape[0]), np.squeeze(labels)] = 1\n",
    "    return categorical\n",
    "\n",
    "def load_mnist_data(raw=False):\n",
    "    \"\"\"Loads MNIST dataset from `DATA_PATH` or downloads it if necessary.\n",
    "\n",
    "    :param raw: `True` if no preprocessing should be applied to the data. Otherwise, data is normalized to 1.\n",
    "    :type raw: `bool`\n",
    "    :return: `(x_train, y_train), (x_test, y_test), min, max`\n",
    "    :rtype: `(np.ndarray, np.ndarray), (np.ndarray, np.ndarray), float, float`\n",
    "    \"\"\"\n",
    "    # from art import DATA_PATH\n",
    "    \n",
    "    path = get_file1('mnist.npz', url='https://s3.amazonaws.com/img-datasets/mnist.npz')\n",
    "\n",
    "    f = np.load(path)\n",
    "    x_train = f['x_train']\n",
    "    y_train = f['y_train']\n",
    "    x_test = f['x_test']\n",
    "    y_test = f['y_test']\n",
    "    f.close()\n",
    "\n",
    "    # Add channel axis\n",
    "    min_, max_ = 0, 255\n",
    "    if not raw:\n",
    "        min_, max_ = 0., 1.\n",
    "        x_train = np.expand_dims(x_train, axis=3)\n",
    "        x_test = np.expand_dims(x_test, axis=3)\n",
    "        x_train, y_train = preprocess(x_train, y_train)\n",
    "        x_test, y_test = preprocess(x_test, y_test)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test), min_, max_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2 Load and prepare dataset for training\n",
    "Load pre-shuffled **MNIST** (*Modified National Institute of Standards and Technology*) annotated data into train and test datasets and create our CNN model to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read MNIST dataset\n",
    "#(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset(str('mnist'))\n",
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_mnist_data()\n",
    "print(\"There are {} images in the training set, and {} in the test set\".format(len(x_train),len(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the first 30 images on 15 columns\n",
    "We use one the functions defined above to get an idea of how the images from the MNIST dataset look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show first thirty images from the training set\n",
    "showImages(x_train,30,15,\"Example: first 30 original images from Training Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Create Keras convolutional neural network\n",
    "This uses a basic CNN architecture from the Keras examples.\n",
    "The model layers are added sequentially:\n",
    "\n",
    "**[Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D)**\n",
    "    It is a 2D convolutional layer that we use to process the 2D MNIST input images:\n",
    "    * The first argument passed to the Conv2D() layer function is the number of output channels â€“ in this case we have 32 output channels. \n",
    "    * `kernel_size`, is the size of the sampling moving window, here a 3x3 aperture\n",
    "    * the activation function is a rectified linear unit (ReLU) \n",
    "    * The shape (size: height, width, color depth) of the input is inferred from the training set image size.\n",
    "    Noe that declaring the input shape is only required of the first layer, Keras will then work out the size of the tensors flowing through the model (automatic shape inference). We start with 26x26 pixels and 32 bit colors\n",
    "\n",
    "**[MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D)**:\n",
    "    Reduction matrix, here we halve each dimension, resulting shape will be a 13x13x32 matrix\n",
    "\n",
    "**[Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten)**:\n",
    "    Transform the input's shape n*c*h*w into a n*(c*h*w) vector) yields 13*13*32=5408 outputs as a flat vector\n",
    "\n",
    "**[Dense(128,relu)](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)**:\n",
    "    A layer where each neuron is linked to each ones of the next layer. This layer is set to yield 128 outputs\n",
    "\n",
    "**[DropOut](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DropOut)**:\n",
    "    Freeze some neurons to avoid overfitting. *Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting.*\n",
    "\n",
    "**[Dense(10,SoftMax)]**\n",
    "    Layer to transform a scoring distribution into a probability distribution, with 10 outputs for our 10 digits. *The [softmax](https://www.tensorflow.org/api_docs/python/tf/keras/activations/softmax) function normalizes ('squashes') a K-dimensional vector of real values to a K-dimensional vector of real values, where each entry is in the range (0, 1),[a] and all the entries add up to 1.\n",
    "\n",
    "**Optimizer parameters**:\n",
    "    * lr: Learning rate, interval used for the gradient descent algorithm\n",
    "    * **[loss]()**: Cost/loss function used for this neural network, here we use *Categorical cross entropy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Keras convolutional neural network - basic architecture from Keras examples\n",
    "k.set_learning_phase(1)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Set Hyperparameters\n",
    "Optimizer='adam'\n",
    "#Optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Initialize and compile:\n",
    "model.compile(optimizer=Optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After execution, we display a summary of our ANN model as shown in the cell result above.\n",
    "Note the total number of weights & bias to be determined during the training sequence and understand how complex can be to train very complex ANN with hundreds of layers.\n",
    "Here we have 3 layers amounting to 320 + 692352 + 1290 = 693962 parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: Build,test & run the CNN model\n",
    "Now we are ready to train our defined model against the MNIST training data.\n",
    "\n",
    "We train the model on test set.   \n",
    "The processing time will depending on the configuration of your environment.\n",
    "You will see the various iterations and epochs with an accuracy & loss values associated to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the Classifier through the ART wrapper\n",
    "Original_Classifier = KerasClassifier(model)\n",
    "Original_Classifier.fit(x_train, y_train, batch_size=128, nb_epochs=10)\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Baseline Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: Analyse results\n",
    "Finally let's test which digits are properly or not recognized by our generated model.   \n",
    "We run `predict`ions on the images present in the test set and compare the predicted digit with the actual (annotated) one\n",
    "\n",
    "We show the 30 first wrongly predicted images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the classifier against fresh test data\n",
    "predOriOri = Original_Classifier.predict(x_test)\n",
    "# count false predictions and display 30 first mismatches\n",
    "AccOriOri = showImagesAccuracy('Showing first 30 test images that are wrongly classified by Keras original classifier, and actual->predicted',x_test, y_test, predOriOri, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Enhancing the model with ART\n",
    "\n",
    "Thanks to the ART toolbox we have a full set of attacks & defense available to assess our classifier.\n",
    "The ART library contains implementations of the following evasion attacks:\n",
    "    * DeepFool (Moosavi-Dezfooli et al., 2015)\n",
    "    * Fast Gradient Method (Goodfellow et al., 2014)\n",
    "    * Basic Iterative Method (Kurakin et al., 2016)\n",
    "    * Projected Gradient Descent (Madry et al., 2017)\n",
    "    * Jacobian Saliency Map (Papernot et al., 2016)\n",
    "    * Universal Perturbation (Moosavi-Dezfooli et al., 2016)\n",
    "    * Virtual Adversarial Method (Miyato et al., 2015)\n",
    "    * C&W Attack (Carlini and Wagner, 2016)\n",
    "    * NewtonFool (Jang et al., 2017)\n",
    "\n",
    "We will now start challenging our Hand Digit recognition model.\n",
    "\n",
    "We'll use one of the above implementations, the Fast Gradient Method and assess our model against it.\n",
    "MNIST training dataset is made of 60000 samples so to save some time let's create only 10% of modified data from it.\n",
    "\n",
    "We apply the `FastGradientMethod` from ART toolbox to the MNIST images and then evaluate how our model scores using the original classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create modified train sub dataset (100 first images) with noise on it.\n",
    "# Craft adversarial samples with the FastGradient Method\n",
    "adv_crafter_FGM = FastGradientMethod(Original_Classifier, eps=0.5)\n",
    "\n",
    "# generate one tenth of images for training set and full for test set\n",
    "x_train_adv_FGM = adv_crafter_FGM.generate(x_train[:(len(x_train)//10)])\n",
    "x_test_adv_FGM = adv_crafter_FGM.generate(x_test[:len(x_test)])\n",
    "showImages(x_train_adv_FGM,30,15,\"Example of first 30 images after applying the FastGradient perturbations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated the perturbated images, let's assess our original classifier against this new training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge the Classifier with FastGradient modified dataset\n",
    "predFGMOri = Original_Classifier.predict(x_test_adv_FGM[:len(x_test_adv_FGM)])\n",
    "AccFGMOri=showImagesAccuracy('Showing first 30 perturbated images that are wrongly classified by Keras original classifier, and actual->predicted',x_test_adv_FGM, y_test, predFGMOri, 30,15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of our model goes down from above 98% accuracy to less than 8%\n",
    "\n",
    "## Step 7: Enhancing the performance of classification\n",
    "You can refer to the ART gitHub writeup to get a full sample on how to train more efficiently your classifier.\n",
    "\n",
    "In this tutorial, we'll explore only one, so that we can observe rapidly ART's added value.\n",
    "\n",
    "To do so let's enrich our initial training data with the one we just created for the Fast Method Gradient Attack, and then retrain the same ANN model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation: expand the training set with the adversarial samples\n",
    "x_train_robust = np.append(x_train, x_train_adv_FGM, axis=0)\n",
    "y_train_robust = np.append(y_train, y_train[:len(x_train_adv_FGM)], axis=0)\n",
    "x_test_robust = np.append(x_test, x_test_adv_FGM[:len(x_test_adv_FGM)], axis=0)\n",
    "y_test_robust = np.append(y_test, y_test[:len(x_test_adv_FGM)], axis=0)\n",
    "\n",
    "# Retrain the CNN on the extended dataset\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# and train a classifier\n",
    "Robust_Classifier = KerasClassifier(model)\n",
    "Robust_Classifier.fit(x_train_robust, y_train_robust, nb_epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Check retrained classifier on modified dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Challenge the Robust Classifier with FastGradient modified dataset\n",
    "predFGMRob = Robust_Classifier.predict(x_test_adv_FGM[:len(x_test_adv_FGM)])\n",
    "AccFGMRob = showImagesAccuracy('Showing first 30 perturbated images that are wrongly classified by Keras retrained classifier, and actual->predicted',x_test_adv_FGM, y_test, predFGMRob, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Check retrained classifier on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Robust Classifier with original dataset\n",
    "predOriRob = Robust_Classifier.predict(x_test)\n",
    "# count false predictions and display 30 first mismatches\n",
    "AccOriRob = showImagesAccuracy('Showing first 30 original images that are wrongly classified by Keras retrained classifier, and actual->predicted',x_test, y_test, predOriRob, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
