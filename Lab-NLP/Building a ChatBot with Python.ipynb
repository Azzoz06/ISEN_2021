{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "___\n\n___\n# Question and Answer Chat Bots"}, {"metadata": {}, "cell_type": "markdown", "source": "----\n\nThe objective of this Notebook is to build a simple Chat Bot that answer simple questions based on a model trained with provided data. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Loading the Data\n\nWe will be working with the Babi Data Set from **Facebook Research**.\n\nFull Details: https://research.fb.com/downloads/babi/\n\n- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n  \"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\",\n  http://arxiv.org/abs/1502.05698\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Data is in Pickle (compressed format), so some imports are needed to manage those training and testing data. "}, {"metadata": {}, "cell_type": "code", "source": "import pickle\nimport numpy as np", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "If you are running this notebook on your own environment, the following two cells will load the needed files.   \n\nIf you are running on a cloud-based environment such as **IBM Watson Studio** you need to load the files from the **Cloud Object Storage**"}, {"metadata": {}, "cell_type": "code", "source": "with open(\"data/train_qa.txt\", \"rb\") as fp:   # Unpickling\n    train_data =  pickle.load(fp)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "with open(\"data/test_qa.txt\", \"rb\") as fp:   # Unpickling\n    test_data =  pickle.load(fp)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The notebook in **Watson Studio** has the functionality to allow you to insert auto-generated code to read `.csv` files. However, if you upload any other types of file, it will not auto-generate the code. To read the file you will likely insert a `StreamingBody object` or insert a sparksession setup. \n\nTo upload a pickle file, select the file in the `Find and add data` side pane, and click **Insert to code** and **StreamingBody object**. This will add the credentials to access the **Cloud Object Storage** and create a StreamingBody object."}, {"metadata": {}, "cell_type": "code", "source": "## Insert code here", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, you have a `StreamingBody object` that is simply an HTTP response that the boto client returns.\n\nRead the object into memory using the following command:"}, {"metadata": {}, "cell_type": "code", "source": "readrawdata = streaming_body_1.read()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Convert the object to `BytesIO` to be able to read it using Pickle Connector or any other connector. For example, you might want to read the Excel file using xlrd. \n\nComplete these steps: "}, {"metadata": {}, "cell_type": "code", "source": "from io import BytesIO\n\ntest_data = pickle.load(BytesIO(readrawdata))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**REDO THE SAME FOR THE `train_qa.txt` pickle file to load into the `train_data` variable.**"}, {"metadata": {}, "cell_type": "markdown", "source": "----"}, {"metadata": {}, "cell_type": "markdown", "source": "## Exploring the Format of the Data"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's look at the type of the data and its length."}, {"metadata": {}, "cell_type": "code", "source": "type(test_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "type(train_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(test_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(train_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We have more or less a 10:1 ratio between training data and testing data. \nLet's take a look at training data. "}, {"metadata": {}, "cell_type": "code", "source": "train_data[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Looking at the data we can see the main three components: \n+ The story: `['Mary','moved','to','the','bathroom','.','Sandra','journeyed','to','the','bedroom','.']` \n+ The question: `['Is', 'Sandra', 'in', 'the', 'hallway', '?']` \n+ The answer: `no` \n\nUse the `join` functionality to format it a but nicer. "}, {"metadata": {}, "cell_type": "code", "source": "' '.join(train_data[0][0])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "' '.join(train_data[0][1])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_data[0][2]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "-----\n\n## Setting up Vocabulary of All Words\n\nTo begin with, we will setup a **vocabulary** of all the words within our data set, and to do this we make sure that we take not just the training data, but also the test data into account. That garantees that when testing our model, it doesn't get confused by maybe new names that did not show up in the training data. "}, {"metadata": {}, "cell_type": "code", "source": "# Create a set that holds the vocab words\nvocab = set()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Remember, test data and train data are just huge lists of tuples where each tuple is three of `story`, `question`, `answer`. \n\nSo what we're doing is just a giant list with a bunch of tuples in it.\n\nSo now if I check out my length of all my data, it should now be eleven thousand."}, {"metadata": {}, "cell_type": "code", "source": "all_data = test_data + train_data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(all_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We want a set of all the unique words. \n> A set in Python is an unordered collection of unique elements. "}, {"metadata": {}, "cell_type": "code", "source": "for story, question, answer in all_data:\n    # In case you don't know what a union of sets is:\n    # https://www.programiz.com/python-programming/methods/set/union\n    vocab = vocab.union(set(story))\n    vocab = vocab.union(set(question))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "vocab.add('no')\nvocab.add('yes')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "vocab", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "vocab_len = len(vocab) + 1 #we add an extra space to hold a 0 for Keras's pad_sequences\nprint(vocab_len)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Actually there is not so many words in our data, so we might be limited we testing our bot. \n\nHow long is the longest story? This is going to be needed later on for padding our sequences."}, {"metadata": {}, "cell_type": "code", "source": "max_story_len = max([len(data[0]) for data in all_data])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "max_story_len", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "max_question_len = max([len(data[1]) for data in all_data])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "max_question_len", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Vectorizing the Data\n\nIn this section we'll vectorize our data. "}, {"metadata": {}, "cell_type": "code", "source": "vocab", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Reserve 0 for pad_sequences\nvocab_size = len(vocab) + 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "-----------"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's import from **Keras** some functions that will help us to tokenize our vocab entries. "}, {"metadata": {}, "cell_type": "code", "source": "from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# integer encode sequences of words\ntokenizer = Tokenizer(filters=[])\ntokenizer.fit_on_texts(vocab)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "tokenizer.word_index", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "After running the **Tokenizer**, we have our tokenized text. Basically it mapped every single entry with a specific index number. \n> Notice as well that every word has been lowercase in the process.  \n\nNext we'll vectorize the story, question and answer in a similar way. That is build a list that contains word indexes rather than the word itself. "}, {"metadata": {}, "cell_type": "code", "source": "train_story_text = []\ntrain_question_text = []\ntrain_answers = []\n\nfor story,question,answer in train_data:\n    train_story_text.append(story)\n    train_question_text.append(question)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_story_seq = tokenizer.texts_to_sequences(train_story_text)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(train_story_text)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "len(train_story_seq)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#print(train_story_text[1])\n#print(train_story_seq[1])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Functionalize Vectorization"}, {"metadata": {}, "cell_type": "markdown", "source": "Here is a function that does all the preprocessing of vectorization for us based on input parameters. \n> Note that we use the max length variable in order to pad the questions to the maximum story length (same for answers). The reason for that is that Keras works with objects of the same length, so we might need to pad to a given length adding *zeros*. \n\nTake some time to read and understand the function below. "}, {"metadata": {}, "cell_type": "code", "source": "def vectorize_stories(data, word_index=tokenizer.word_index, max_story_len=max_story_len,max_question_len=max_question_len):\n    '''\n    INPUT: \n    \n    data: consisting of Stories,Queries,and Answers\n    word_index: word index dictionary from tokenizer\n    max_story_len: the length of the longest story (used for pad_sequences function)\n    max_question_len: length of the longest question (used for pad_sequences function)\n\n\n    OUTPUT:\n    \n    Vectorizes the stories,questions, and answers into padded sequences. We first loop for every story, query , and\n    answer in the data. Then we convert the raw words to an word index value. Then we append each set to their appropriate\n    output list. Then once we have converted the words to numbers, we pad the sequences so they are all of equal length.\n    \n    Returns this in the form of a tuple (X,Xq,Y) (padded based on max lengths)\n    '''\n    \n    \n    # X = STORIES\n    X = []\n    # Xq = QUERY/QUESTION\n    Xq = []\n    # Y = CORRECT ANSWER\n    Y = []\n    \n    \n    for story, query, answer in data:\n        \n        # Grab the word index for every word in story\n        x = [word_index[word.lower()] for word in story]\n        # Grab the word index for every word in query\n        xq = [word_index[word.lower()] for word in query]\n        \n        # Grab the Answers (either Yes/No so we don't need to use list comprehension here)\n        # Index 0 is reserved so we're going to use + 1\n        y = np.zeros(len(word_index) + 1)\n        \n        # Now that y is all zeros and we know its just Yes/No , we can use numpy logic to create this assignment\n        #\n        y[word_index[answer]] = 1\n        \n        # Append each set of story, question, and answer to their respective holding lists\n        X.append(x)\n        Xq.append(xq)\n        Y.append(y)\n        \n    # Finally, pad the sequences based on their max length so the RNN can be trained on uniformly long sequences.\n        \n    # RETURN TUPLE FOR UNPACKING\n    return (pad_sequences(X, maxlen=max_story_len),pad_sequences(Xq, maxlen=max_question_len), np.array(Y))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use the above function to vectorize our training and testing data sets. "}, {"metadata": {}, "cell_type": "code", "source": "inputs_train, queries_train, answers_train = vectorize_stories(train_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "inputs_test, queries_test, answers_test = vectorize_stories(test_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "inputs_test", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "queries_test", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "answers_test", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sum(answers_test)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "tokenizer.word_index['yes']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "tokenizer.word_index['no']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "At this point we have successfully vectorized our stories, questions and answers and we can consider the data being in the correct format for creating the model which we'll do next using Keras layers. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Creating the Model"}, {"metadata": {}, "cell_type": "markdown", "source": "> A quick note as a reminder. It is important that you read the paper provided in the resources since it's going to be fundamental to understanding how the network and the encoders work. \n\nWe'll now start building out the neural network. Here's essentially the diagram of the network that we're producing along the encoders and the LSTM unit or the RNN that's used from the paper. \n\n<img src='../Resources/OverallModel.png' width=600/>"}, {"metadata": {}, "cell_type": "code", "source": "from keras.models import Sequential, Model\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Input, Activation, Dense, Permute, Dropout\nfrom keras.layers import add, dot, concatenate\nfrom keras.layers import LSTM", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Placeholders for Inputs\n\nRecall we technically have two inputs, stories and questions. So we need to use placeholders. `Input()` is used to instantiate a Keras tensor, and we need to pass in a shape which is going to be based on the max story length and the max question length.\n"}, {"metadata": {}, "cell_type": "code", "source": "input_sequence = Input((max_story_len,))\nquestion = Input((max_question_len,))", "execution_count": null, "outputs": []}, {"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "### Building the Networks\n\nTo understand why we chose this setup, make sure to read the paper we are using:\n\n* Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n  \"End-To-End Memory Networks\",\n  http://arxiv.org/abs/1503.08895"}, {"metadata": {}, "cell_type": "markdown", "source": "## Encoders\n\n### Input Encoder m"}, {"metadata": {}, "cell_type": "code", "source": "# Reserve 0 for pad_sequences\nvocab_size = len(vocab) + 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's create our input Encoder `m` as refered in the paper with two layers:\n1. The **Embedding layer** with an input dimension equal to our vocab size and an output dimension of **64** \n2. A **Dropout layer** which drops a percentage of neurons, in our case 30 percent of the neurons are going to be randomly turned off during the training. This will help prefenting **overfitting**."}, {"metadata": {}, "cell_type": "code", "source": "# Input gets embedded to a sequence of vectors\ninput_encoder_m = Sequential()\ninput_encoder_m.add(Embedding(input_dim=vocab_size,output_dim=64))\ninput_encoder_m.add(Dropout(0.3))\n\n# This encoder will output:\n# (samples, story_maxlen, embedding_dim)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Input Encoder c"}, {"metadata": {}, "cell_type": "markdown", "source": "We'll do pretty much the same with Encoder `c` with some differences for the **output dimension**."}, {"metadata": {}, "cell_type": "code", "source": "# embed the input into a sequence of vectors of size query_maxlen\ninput_encoder_c = Sequential()\ninput_encoder_c.add(Embedding(input_dim=vocab_size,output_dim=max_question_len))\ninput_encoder_c.add(Dropout(0.3))\n# output: (samples, story_maxlen, query_maxlen)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Question Encoder"}, {"metadata": {}, "cell_type": "markdown", "source": "And we do the same for the question Encoder. "}, {"metadata": {}, "cell_type": "code", "source": "# embed the question into a sequence of vectors\nquestion_encoder = Sequential()\nquestion_encoder.add(Embedding(input_dim=vocab_size,\n                               output_dim=64,\n                               input_length=max_question_len))\nquestion_encoder.add(Dropout(0.3))\n# output: (samples, query_maxlen, embedding_dim)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Encode the Sequences"}, {"metadata": {}, "cell_type": "markdown", "source": "Now that we have the **input encoder m**, **input encoder c** and the **question encoder**, it's time to actually encode the sequences."}, {"metadata": {}, "cell_type": "code", "source": "# encode input sequence and questions (which are indices)\n# to sequences of dense vectors\ninput_encoded_m = input_encoder_m(input_sequence)\ninput_encoded_c = input_encoder_c(input_sequence)\nquestion_encoded = question_encoder(question)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "##### Use dot product to compute the match between first input vector seq and the query (the question encoded). \nRefer to the paper section 2.1. The `softmax` activation function gives the probability vector over the inputs. "}, {"metadata": {}, "cell_type": "code", "source": "# shape: `(samples, story_maxlen, query_maxlen)`\nmatch = dot([input_encoded_m, question_encoded], axes=(2, 2))\nmatch = Activation('softmax')(match)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Add this match matrix with the second input vector sequence"}, {"metadata": {}, "cell_type": "code", "source": "# add the match matrix with the second input vector sequence\nresponse = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\nresponse = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### Concatenate\n\nSo now that we have a response, we can concatenate the match matrix with the question vector sequence."}, {"metadata": {}, "cell_type": "code", "source": "# concatenate the match matrix with the question vector sequence\nanswer = concatenate([response, question_encoded])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "answer", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Reduce with RNN (LSTM)\nanswer = LSTM(32)(answer)  # (samples, 32)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Regularization with Dropout\nanswer = Dropout(0.5)(answer)\nanswer = Dense(vocab_size)(answer)  # (samples, vocab_size)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# we output a probability distribution over the vocabulary\nanswer = Activation('softmax')(answer)\n\n# build the final model\nmodel = Model([input_sequence, question], answer)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n              metrics=['accuracy'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model.summary()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Training the model\n\nWe now have a defined model, all we need to do now is to train this model, evaluate it and evaluate the model on the given test data as well as write our own stories and questions to see how it performs. \n\n> Note that we use the training input stories and questions to train the model. We set the epochs on 120, note that each epoch is going to take between 5 to 10 seconds, so the training is going to take some time. You can set the epochs to whatever number. \n\nYou don't have to run the fitting on large set of epochs if you don't want to. You can load pre-trained models for you that we have already trained, saved and provided for you. "}, {"metadata": {}, "cell_type": "code", "source": "# train\nhistory = model.fit([inputs_train, queries_train], answers_train,batch_size=32,epochs=120,validation_data=([inputs_test, queries_test], answers_test))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Saving the Model"}, {"metadata": {}, "cell_type": "code", "source": "filename = 'data/chatbot_120_epochs.h5'\nmodel.save(filename)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Evaluating the Model\n\n### Plotting Out Training History"}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\n%matplotlib inline\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Looking at the above plot, we realize that accuracy starts to drastically improve at about 15 epochs. You can play around with the plotting options to plot the loss or any other metric. \n\n### Evaluating on Given Test Set\n\nIn this step we are going to predict the answer using the `model.predict` function and passing it a tuple with two inputs:\n1. the inputs test\n2. the queries test  \n\nThere is no need to pass the answer because that is actually what we are trying to predict based on the test set given as input.  \nAt this point you can provide your own stories and questions and see the predicted answer. We'll do this step later on. "}, {"metadata": {}, "cell_type": "code", "source": "model.load_weights(filename)\npred_results = model.predict(([inputs_test, queries_test]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As a reminder, let's take another fresh at our test data. "}, {"metadata": {}, "cell_type": "code", "source": "test_data[0][0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "story =' '.join(word for word in test_data[0][0])\nprint(story)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "query = ' '.join(word for word in test_data[0][1])\nprint(query)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"True Test Answer from Data is:\",test_data[0][2])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The below output show the predicted results and the probability for random words like John, kitchen and so on from our test set based off the tokenized or word index. The values should be mostly pretty low as expected as we don't expect the correct answer to be something like 'John' or 'milk' but more a **yes** or **no**.  \nLooking at the below probability, there is a word index which has a probability close to 99%, I bet this is our **predicted answer**.  "}, {"metadata": {}, "cell_type": "code", "source": "pred_results[0]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's perform a `argmax` on the predicted results to only retain the **max probability** and generate directly the value (yes/no) from it. "}, {"metadata": {}, "cell_type": "code", "source": "#Generate prediction from model\nval_max = np.argmax(pred_results[0])\n\nfor key, val in tokenizer.word_index.items():\n    if val == val_max:\n        k = key\n\nprint(\"Predicted answer is: \", k)\nprint(\"Probability of certainty was: \", pred_results[0][val_max])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Writing Your Own Stories and Questions\n\nThis is the cool part where we write our own stories and questions and see how the model performs.  \n\n**Remember you can only use words from the existing vocab and use correct formating.**"}, {"metadata": {}, "cell_type": "code", "source": "vocab", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Note the whitespace of the periods\nmy_story = \"John left the kitchen . Sandra dropped the football in the garden .\"\nmy_story.split()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "my_question = \"Is the football in the garden ?\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "my_question.split()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's format our input data (own story and question) as it is meant to be when we trained the model, that is using the `split()` function and **vectorize our inputs**, and provide the correct answer which is **yes** cause Sandra actually dropped the football in the garden.  \n> Note: although we provide the answer for formating purpose, we'll let the model predict it for us. "}, {"metadata": {}, "cell_type": "code", "source": "mydata = [(my_story.split(),my_question.split(),'yes')]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "my_story,my_ques,my_ans = vectorize_stories(mydata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pred_results = model.predict(([ my_story, my_ques]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Generate prediction from model\nval_max = np.argmax(pred_results[0])\n\nfor key, val in tokenizer.word_index.items():\n    if val == val_max:\n        k = key\n\nprint(my_question)\nprint(\"Predicted answer is: \", k)\nprint(\"Probability of certainty was: \", pred_results[0][val_max])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Great Job!\n\nYou have completed this lab and I hope you have enjoyed it and have understood the approach. As an optional exercice you can provide a series of additional stories and questions and see how the model predicts.  \n\nYou can change the question and re-run the cell to see the changes in the predicted answer. "}, {"metadata": {}, "cell_type": "code", "source": "#Provide a story\nmy_story2 = \"Daniel went to the office . Mary left the apple in the bedroom .\"\nmy_story2.split()\n#Provide a question\nmy_question2 = \"Is the apple in the kitchen ?\"\nmy_question2.split()\nmydata2 = [(my_story2.split(),my_question2.split(),'yes')]\nmy_story2,my_ques2,my_ans2 = vectorize_stories(mydata2)\npred_results = model.predict(([ my_story2, my_ques2]))\n\n#Generate prediction from model\nval_max = np.argmax(pred_results[0])\n\nfor key, val in tokenizer.word_index.items():\n    if val == val_max:\n        k = key\n\nprint(my_question2)\nprint(\"Predicted answer is: \", k)\nprint(\"Probability of certainty was: \", pred_results[0][val_max])", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}