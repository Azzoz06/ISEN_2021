{"cells": [{"metadata": {"collapsed": true}, "cell_type": "markdown", "source": "___\n\n\n___"}, {"metadata": {}, "cell_type": "markdown", "source": "# Sentiment Analysis\nIn this Hands-on lab we will start to investigate sentiment analysis using code. The goal is to find commonalities between documents, with the understanding that similarly *combined* vectors should correspond to similar sentiments.\n\nWhile the scope of sentiment analysis is very broad, we will focus our work in two ways.\n\n### 1. Polarity classification\nWe won't try to determine if a sentence is objective or subjective, fact or opinion. Rather, we care only if the text expresses a *positive*, *negative* or *neutral* opinion.\n### 2. Document level scope\nWe'll also try to aggregate all of the sentences in a document or paragraph, to arrive at an overall opinion.\n### 3. Coarse analysis\nWe won't try to perform a fine-grained analysis that would determine the degree of positivity/negativity. That is, we're not trying to guess how many stars a reviewer awarded, just whether the review was positive or negative."}, {"metadata": {}, "cell_type": "markdown", "source": "## Broad Steps:\n* First, consider the text being analyzed. A model trained on paragraph-long movie reviews might not be effective on tweets. Make sure to use an appropriate model for the task at hand.\n* Next, decide the type of analysis to perform. In the previous section on text classification we used a bag-of-words technique that considered only single tokens, or *unigrams*. Some rudimentary sentiment analysis models go one step further, and consider two-word combinations, or *bigrams*. In this section, we'd like to work with complete sentences, and for this we're going to import a trained NLTK lexicon called *VADER*."}, {"metadata": {}, "cell_type": "markdown", "source": "## NLTK's VADER module\nVADER is an NLTK module that provides sentiment scores based on words used (\"completely\" boosts a score, while \"slightly\" reduces it), on capitalization & punctuation (\"GREAT!!!\" is stronger than \"great.\"), and negations (words like \"isn't\" and \"doesn't\" affect the outcome).\n<br>To view the source code visit https://www.nltk.org/_modules/nltk/sentiment/vader.html"}, {"metadata": {}, "cell_type": "markdown", "source": "**Download the VADER lexicon.** You only need to do this once."}, {"metadata": {}, "cell_type": "code", "source": "import nltk\nnltk.download('vader_lexicon')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<div class=\"alert alert-danger\">NOTE: At the time of this writing there's a <a href='https://github.com/nltk/nltk/issues/2053'>known issue</a> with SentimentIntensityAnalyzer that raises a harmless warning on loading<br>\n<tt><font color=black>&emsp;UserWarning: The twython library has not been installed.<br>&emsp;Some functionality from the twitter package will not be available.</tt>\n\nThis is due to be fixed in an upcoming NLTK release. For now, if you want to avoid it you can (optionally) install the NLTK twitter library with<br>\n<tt><font color=black>&emsp;conda install nltk[twitter]</tt><br>or<br>\n<tt><font color=black>&emsp;pip3 install -U nltk[twitter]</tt></div>"}, {"metadata": {}, "cell_type": "code", "source": "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nsid = SentimentIntensityAnalyzer()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "VADER's `SentimentIntensityAnalyzer()` takes in a string and returns a dictionary of scores in each of four categories:\n* negative\n* neutral\n* positive\n* compound *(computed by normalizing the scores above)*"}, {"metadata": {}, "cell_type": "code", "source": "a = 'This was a good movie.'\nsid.polarity_scores(a)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "a = 'This was the best, most awesome movie EVER MADE!!!'\nsid.polarity_scores(a)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "a = 'This was the worst film to ever disgrace the screen.'\nsid.polarity_scores(a)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Use VADER to analyze Amazon Reviews\nFor this exercise we're going to apply `SentimentIntensityAnalyzer` to a dataset of 10,000 Amazon reviews. Like our movie reviews datasets, these are labeled as either \"pos\" or \"neg\". At the end we'll determine the accuracy of our sentiment analysis with VADER."}, {"metadata": {}, "cell_type": "markdown", "source": "### Loading data\nWe will be using the `amazonreviews.tsv` file for this lab. Whether you are running on **IBM Watson Studio** or on your local **Anaconda** environment, the way to load data differs.  \nRun one of the two cells below to load the data into a `Dataframe`. \n> **Note:** if you are running on IBM Watson Studio, make sure you have imported the file in the data asset section and then click on **Import as Pandas Data Frame**, the tool will automatically create the import statement from the IBM Cloud Object Storage into a Pandas DataFrame. "}, {"metadata": {}, "cell_type": "code", "source": "# IBM Watson Studio Code\n\n# Insert into code the amazonreview.tsv file as pandas DataFrame\n\n", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Anaconda or Local Environment\n\nimport numpy as np\nimport pandas as pd\n\ndf = pd.read_csv('../TextFiles/amazonreviews.tsv', sep='\\t')\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df['label'].value_counts()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Clean the data (optional):\nRecall that our moviereviews.tsv file contained empty records. Let's check to see if any exist in amazonreviews.tsv."}, {"metadata": {}, "cell_type": "code", "source": "# REMOVE NaN VALUES AND EMPTY STRINGS:\ndf.dropna(inplace=True)\n\nblanks = []  # start with an empty list\n\nfor i,lb,rv in df.itertuples():  # iterate over the DataFrame\n    if type(rv)==str:            # avoid NaN values\n        if rv.isspace():         # test 'review' for whitespace\n            blanks.append(i)     # add matching index numbers to the list\n\ndf.drop(blanks, inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df['label'].value_counts()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In this case there were no empty records. Good!"}, {"metadata": {}, "cell_type": "markdown", "source": "## Let's run the first review through VADER"}, {"metadata": {}, "cell_type": "code", "source": "sid.polarity_scores(df.loc[0]['review'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df.loc[0]['label']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Great! Our first review was labeled \"positive\", and earned a positive compound score."}, {"metadata": {}, "cell_type": "markdown", "source": "## Adding Scores and Labels to the DataFrame\nIn this next section we'll add columns to the original DataFrame to store polarity_score dictionaries, extracted compound scores, and new \"pos/neg\" labels derived from the compound score. We'll use this last column to perform an accuracy test."}, {"metadata": {}, "cell_type": "code", "source": "df['scores'] = df['review'].apply(lambda review: sid.polarity_scores(review))\n\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound'])\n\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df['comp_score'] = df['compound'].apply(lambda c: 'pos' if c >=0 else 'neg')\n\ndf.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Report on Accuracy\nFinally, we'll use scikit-learn to determine how close VADER came to our original 10,000 labels."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import accuracy_score,classification_report,confusion_matrix", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "accuracy_score(df['label'],df['comp_score'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(df['label'],df['comp_score']))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(confusion_matrix(df['label'],df['comp_score']))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "This tells us that VADER correctly identified an Amazon review as \"positive\" or \"negative\" roughly 71% of the time.\n## Up Next: Sentiment Analysis Project"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.8", "language": "python"}, "language_info": {"name": "python", "version": "3.8.12", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}